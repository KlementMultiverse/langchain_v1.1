# Agentic AI Mastery - 2025 Curriculum

**Based on:** Anthropic's "Building Effective Agents" + Industry Best Practices

**Goal:** Master agentic AI design from simple workflows to autonomous multi-agent systems

**Duration:** 12 weeks (1 hour/day) - Progressive mastery

**Philosophy:** Start simple. Build with composable patterns. Progress to complexity.

---

## ðŸŽ¯ Core Principle (Anthropic 2024)

> "The most successful implementations weren't using complex frameworks or specialized libraries, but instead were building with simple, composable patterns."

**Your Learning Path:**
1. Master pseudocode planning (think before code)
2. Learn Anthropic's 5 core patterns (simple â†’ complex)
3. Build with frameworks (LangGraph, AutoGen, CrewAI)
4. Design autonomous agentic systems

---

## ðŸ“š Curriculum Structure

### **Phase 1: Foundations + Pseudocode** (Weeks 1-2)
Master planning and simple patterns

### **Phase 2: Anthropic's 5 Patterns** (Weeks 3-4)
Workflow patterns â†’ Agent patterns

### **Phase 3: Framework Mastery** (Weeks 5-6)
LangGraph, AutoGen, CrewAI

### **Phase 4: Multi-Agent Systems** (Weeks 7-8)
Orchestration, collaboration, coordination

### **Phase 5: Autonomous Agents** (Weeks 9-12)
Self-directed, adaptive, production systems

---

## Phase 1: Foundations + Pseudocode (Weeks 1-2)

### **Day 1: The Backwards Method + Pseudocode Basics**

**Lesson:**
- Work backwards from desired output
- Identify dependencies before coding
- Write pseudocode in plain language

**Concept: Pseudocode Fundamentals**
```
PSEUDOCODE RULES:
1. Use plain language (readable by non-programmers)
2. Focus on WHAT, not HOW
3. Use standard keywords: IF, ELSE, FOR, WHILE, FUNCTION, RETURN
4. Indent to show structure
5. Don't use language-specific syntax
```

**Exercise:**
```
Problem: Build a function that checks if an email is valid

Step 1: What's the output? (Boolean: True/False)

Step 2: What do we need to check?
- Has @ symbol
- Has text before @
- Has domain after @
- Has extension (.com, .org, etc)

Step 3: Write pseudocode (NO CODE YET!)

FUNCTION is_valid_email(email):
    # Check basic structure
    IF email is empty:
        RETURN False

    IF email does not contain "@":
        RETURN False

    # Split email into parts
    parts = SPLIT email by "@"

    IF parts has less than 2 elements:
        RETURN False

    local_part = parts[0]
    domain_part = parts[1]

    # Validate local part
    IF local_part is empty:
        RETURN False

    # Validate domain
    IF domain_part does not contain ".":
        RETURN False

    # All checks passed
    RETURN True
```

**Practice (30 minutes):**
Write pseudocode for these 3 problems (NO CODING):
1. Password strength checker (weak/medium/strong)
2. Shopping cart total calculator (with discounts)
3. Task priority sorter (urgent/normal/low)

**Success Criteria:**
- Pseudocode readable by anyone
- All edge cases identified
- Clear step-by-step logic

---

### **Day 2: Layer Identification + Dependency Mapping**

**Lesson:**
- Identify building blocks (helpers)
- Map dependencies between functions
- Build bottom-up (smallest pieces first)

**Real Example: Our Resume Extractor**
```
GOAL: Load all resume files from folder â†’ return text

LAYER 1 (No dependencies - Pure helpers):
â”œâ”€â”€ get_file_extension(path)
â”‚   â””â”€â”€ Returns: ".pdf", ".docx", ".txt"

LAYER 2 (Uses Layer 1):
â”œâ”€â”€ load_single_resume(path)
â”‚   â”œâ”€â”€ Uses: get_file_extension()
â”‚   â””â”€â”€ Returns: text content

LAYER 3 (Uses Layer 2):
â””â”€â”€ load_all_resumes(folder)
    â”œâ”€â”€ Uses: load_single_resume()
    â””â”€â”€ Returns: dictionary {filename: text}
```

**Pseudocode Practice:**
```
Problem: Build a blog post analyzer
- Input: Blog post text
- Output: Reading time, word count, sentiment, key topics

Your Task:
1. Work backwards from output
2. Identify 3 layers of functions
3. Write pseudocode for each layer
4. Show dependencies with arrows

Example structure:
LAYER 1:
  - count_words(text)
  - extract_sentences(text)
  - calculate_reading_time(word_count)

LAYER 2:
  - analyze_sentiment(sentences)  [uses extract_sentences]
  - extract_topics(text)           [uses count_words]

LAYER 3:
  - analyze_blog_post(text)        [uses all Layer 1 & 2]
```

**Success Criteria:**
- Can identify 3+ distinct layers
- Clear dependency graph
- Bottom-up build order planned

---

### **Day 3: Anthropic Pattern #1 - Prompt Chaining**

**Lesson:**
> "Decomposes a task into a sequence of steps where each LLM call processes the output of the previous one."

**Real-World Example: Our Two-Pass Extraction**
```
Pass 1: Extract raw data (LLM can think/explain)
   â†“
Pass 2: Clean and structure (force format)
   â†“
Validated output

This IS prompt chaining!
```

**Architecture Pattern:**
```
INPUT â†’ LLM_1 â†’ OUTPUT_1 â†’ LLM_2 â†’ OUTPUT_2 â†’ LLM_3 â†’ FINAL_OUTPUT

Each step:
- Takes previous output
- Transforms it
- Passes to next step
```

**Pseudocode Template:**
```pseudocode
FUNCTION prompt_chain(input_text):
    # Step 1: Initial analysis
    analysis = LLM_CALL(
        prompt="Analyze this text: {input_text}",
        model="gpt-4"
    )

    # Step 2: Extract key points (uses Step 1 output)
    key_points = LLM_CALL(
        prompt="From this analysis, extract 5 key points: {analysis}",
        model="gpt-4"
    )

    # Step 3: Summarize (uses Step 2 output)
    summary = LLM_CALL(
        prompt="Create a one-sentence summary: {key_points}",
        model="gpt-4"
    )

    RETURN summary
```

**Exercise (45 minutes):**
```
Problem: Build a research paper analyzer using prompt chaining

Chain Steps:
1. Extract main topic and research question
2. Summarize methodology
3. Extract key findings
4. Generate executive summary

Your Task:
1. Write pseudocode for entire chain
2. Plan what each LLM call does
3. Show data flow between steps
4. Identify where validation happens
5. Plan error handling (what if LLM returns empty?)

NO CODING - Pure planning!
```

**When to Use Prompt Chaining:**
- âœ… Task can be decomposed into fixed steps
- âœ… Each step builds on previous
- âœ… Need higher accuracy (worth latency cost)
- âŒ Don't use for parallel independent tasks

---

### **Day 4: Pattern #2 - Routing**

**Lesson:**
> "Classifies an input and directs it to a specialized followup task."

**Architecture:**
```
                    INPUT
                      â†“
              [ROUTER/CLASSIFIER]
                      â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“             â†“             â†“
    SPECIALIST_A  SPECIALIST_B  SPECIALIST_C
        â†“             â†“             â†“
    OUTPUT_A      OUTPUT_B      OUTPUT_C
```

**Pseudocode Pattern:**
```pseudocode
FUNCTION route_task(user_input):
    # Step 1: Classify the input
    category = LLM_CALL(
        prompt="Classify this request into: technical, sales, support, or general. Request: {user_input}",
        model="gpt-4"
    )

    # Step 2: Route to specialist
    IF category == "technical":
        response = technical_specialist(user_input)
    ELSE IF category == "sales":
        response = sales_specialist(user_input)
    ELSE IF category == "support":
        response = support_specialist(user_input)
    ELSE:
        response = general_assistant(user_input)

    RETURN response

FUNCTION technical_specialist(input):
    # Specialized prompt for technical queries
    RETURN LLM_CALL(
        prompt="As a technical expert, answer: {input}",
        model="gpt-4",
        temperature=0.1  # More precise
    )

FUNCTION sales_specialist(input):
    # Specialized prompt for sales queries
    RETURN LLM_CALL(
        prompt="As a sales expert, answer: {input}",
        model="gpt-4",
        temperature=0.7  # More creative
    )
```

**Exercise (45 minutes):**
```
Problem: Build a document processor with routing

Categories:
- Invoice â†’ Extract: vendor, amount, date, items
- Resume â†’ Extract: contact, skills, experience
- Contract â†’ Extract: parties, terms, dates
- Email â†’ Extract: sender, subject, action items

Your Task:
1. Design the router (how to classify?)
2. Write pseudocode for main router function
3. Write pseudocode for each specialist
4. Plan error handling (what if classification is uncertain?)
5. Design fallback strategy

Example consideration:
What if document is 40% invoice, 60% contract? How to handle?
```

**When to Use Routing:**
- âœ… Distinct categories that need different handling
- âœ… Different prompts/models for different tasks
- âœ… Can clearly classify input
- âŒ Don't use if all inputs handled the same way

---

### **Day 5: Pattern #3 - Parallelization**

**Lesson:**
> "Breaking a task into independent subtasks run in parallel."

**Architecture:**
```
                    INPUT
                      â†“
              [TASK SPLITTER]
                      â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“             â†“             â†“
    TASK_A        TASK_B        TASK_C
    (parallel)    (parallel)    (parallel)
        â†“             â†“             â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
                [AGGREGATOR]
                      â†“
                   OUTPUT
```

**Key Requirement:** Tasks must be **independent** (no dependencies)

**Pseudocode Pattern:**
```pseudocode
FUNCTION parallel_analysis(document):
    # Define independent tasks
    tasks = [
        TASK("sentiment", "Analyze sentiment of: {document}"),
        TASK("topics", "Extract key topics from: {document}"),
        TASK("entities", "Extract named entities from: {document}"),
        TASK("summary", "Summarize in one sentence: {document}")
    ]

    # Run all tasks in parallel
    results = PARALLEL_EXECUTE(tasks)

    # results = {
    #     "sentiment": "positive",
    #     "topics": ["AI", "technology", "innovation"],
    #     "entities": ["OpenAI", "GPT-4", "San Francisco"],
    #     "summary": "The document discusses AI innovations..."
    # }

    # Aggregate results
    final_output = COMBINE(results)

    RETURN final_output
```

**Exercise (45 minutes):**
```
Problem: Build a multi-language documentation generator

Input: Product description (English)
Output: Documentation in 5 languages + SEO keywords + FAQ + Use cases

Tasks (all independent):
1. Translate to Spanish
2. Translate to French
3. Translate to German
4. Translate to Japanese
5. Generate SEO keywords
6. Generate FAQ (5 questions)
7. Generate 3 use cases

Your Task:
1. Write pseudocode for parallel executor
2. Plan how to collect results
3. Design timeout strategy (what if one translation takes too long?)
4. Plan error handling (what if French translation fails?)
5. Design aggregation logic

Consider: Should we continue if 1 task fails? Or cancel all?
```

**When to Use Parallelization:**
- âœ… Tasks are completely independent
- âœ… Need to reduce latency
- âœ… Each task takes significant time
- âŒ Don't use if tasks depend on each other

---

### **Day 6-7: Weekend - Build Your First Patterns**

**Saturday (60 minutes):**
- Review Days 1-5
- Identify which pattern we used in Day 7 Resume Extractor (Hint: Prompt Chaining!)
- Write pseudocode for 3 new problems:
  1. Email responder (Routing pattern)
  2. Product review analyzer (Parallelization)
  3. Content improver (Prompt Chaining)

**Sunday (60 minutes):**
- Speed challenge: 15 minutes per problem
- Plan (pseudocode only) these systems:
  1. Customer support chatbot (which pattern?)
  2. Code review assistant (which pattern?)
  3. Legal document analyzer (which pattern?)
  4. Social media content generator (which pattern?)

**Goal:** Recognize patterns quickly, plan faster

---

### **Week 2: Advanced Patterns + Real Implementation**

### **Day 8: Pattern #4 - Orchestrator-Workers**

**Lesson:**
> "A central LLM dynamically breaks down tasks, delegates them to worker LLMs, and integrates their results."

**Key Difference from Routing:**
- Routing: Fixed categories (pre-defined specialists)
- Orchestrator: Dynamic task breakdown (creates subtasks on the fly)

**Architecture:**
```
                    INPUT
                      â†“
              [ORCHESTRATOR]
             (plans subtasks)
                      â†“
        "I need to do: A, B, C"
                      â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“             â†“             â†“
    WORKER_1      WORKER_2      WORKER_3
    (task A)      (task B)      (task C)
        â†“             â†“             â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
              [ORCHESTRATOR]
            (integrates results)
                      â†“
                   OUTPUT
```

**Pseudocode Pattern:**
```pseudocode
FUNCTION orchestrator_worker(complex_task):
    # Step 1: Orchestrator plans the work
    plan = LLM_CALL(
        prompt="""Break down this task into 3-5 subtasks: {complex_task}

        Return as JSON:
        {
            "subtasks": [
                {"id": 1, "description": "...", "priority": "high"},
                {"id": 2, "description": "...", "priority": "medium"}
            ]
        }""",
        model="gpt-4"
    )

    # Step 2: Execute each subtask with workers
    results = []
    FOR EACH subtask IN plan["subtasks"]:
        result = worker_execute(subtask)
        results.APPEND(result)

    # Step 3: Orchestrator integrates results
    final_output = LLM_CALL(
        prompt="""I broke down a task into subtasks. Here are the results:

        Subtasks: {plan}
        Results: {results}

        Integrate these into a final coherent answer.""",
        model="gpt-4"
    )

    RETURN final_output

FUNCTION worker_execute(subtask):
    # Worker just executes what it's told
    result = LLM_CALL(
        prompt="Complete this subtask: {subtask.description}",
        model="gpt-4"
    )
    RETURN result
```

**Exercise (45 minutes):**
```
Problem: Build a research assistant using orchestrator-workers

Input: "Analyze the impact of AI on healthcare in the last 5 years"

Orchestrator should dynamically decide:
- What aspects to research (AI diagnostics? Drug discovery? Patient care?)
- How many subtasks needed (3? 5? 10?)
- Priority of each subtask
- How to integrate findings

Your Task:
1. Write pseudocode for orchestrator planning phase
2. Write pseudocode for worker execution
3. Write pseudocode for result integration
4. Plan error handling (what if worker fails?)
5. Design retry strategy
6. Consider: Should orchestrator re-plan if a worker fails?
```

**When to Use Orchestrator-Workers:**
- âœ… Complex tasks where subtasks can't be predicted upfront
- âœ… Need dynamic task decomposition
- âœ… Requirements change based on intermediate results
- âŒ Don't use if task structure is fixed (use Prompt Chaining instead)

---

### **Day 9: Pattern #5 - Evaluator-Optimizer (Reflection)**

**Lesson:**
> "One LLM call generates a response while another provides evaluation, which loops back as feedback to iteratively refine output."

**Also Called:** Reflection, Self-Critique, LLM-as-Judge

**Architecture:**
```
    INPUT â†’ [GENERATOR] â†’ draft_v1
                            â†“
              [EVALUATOR] â†’ critique
                            â†“
            [GENERATOR] â†’ draft_v2 (uses critique)
                            â†“
              [EVALUATOR] â†’ critique_v2
                            â†“
              Is it good enough?
              â†“           â†“
             YES         NO â†’ loop again
              â†“
         FINAL OUTPUT
```

**Pseudocode Pattern:**
```pseudocode
FUNCTION evaluator_optimizer(task, max_iterations=3):
    iteration = 0
    current_output = None
    feedback = "Initial attempt"

    WHILE iteration < max_iterations:
        # Generator creates/improves output
        current_output = LLM_CALL(
            prompt="""Task: {task}

            Previous attempt: {current_output}
            Feedback: {feedback}

            Create an improved version.""",
            model="gpt-4"
        )

        # Evaluator critiques the output
        evaluation = LLM_CALL(
            prompt="""Evaluate this output for: {task}

            Output: {current_output}

            Score 1-10 and provide specific feedback.

            Return JSON:
            {
                "score": 8,
                "feedback": "Specific improvements needed...",
                "is_acceptable": true/false
            }""",
            model="gpt-4"
        )

        # Check if good enough
        IF evaluation["is_acceptable"] OR evaluation["score"] >= 8:
            RETURN current_output

        # Prepare for next iteration
        feedback = evaluation["feedback"]
        iteration = iteration + 1

    # Max iterations reached
    RETURN current_output
```

**Exercise (45 minutes):**
```
Problem: Build a code review assistant with reflection

Input: Python function
Output: Improved function + explanation

Evaluation Criteria:
- Code correctness
- Performance (time/space complexity)
- Readability
- Error handling
- Documentation

Your Task:
1. Write pseudocode for generator (code improver)
2. Write pseudocode for evaluator (code reviewer)
3. Define evaluation criteria as a checklist
4. Plan stopping condition (when is code "good enough"?)
5. Design max iterations strategy (don't loop forever!)
6. Consider: Should evaluator see previous critiques? Or fresh eyes each time?

Example flow:
Iteration 1: Original code â†’ "Missing error handling" â†’ Add try-catch
Iteration 2: Improved code â†’ "Could use better variable names" â†’ Rename vars
Iteration 3: Better code â†’ "Score 9/10, acceptable" â†’ Done!
```

**When to Use Evaluator-Optimizer:**
- âœ… Clear evaluation criteria exist
- âœ… Output quality matters more than speed
- âœ… Can define "good enough" threshold
- âŒ Don't use if no objective evaluation criteria

---

### **Day 10: Combining Patterns**

**Lesson:**
Real systems use **multiple patterns together**

**Example: Advanced Document Processor**
```
INPUT: Mixed documents
    â†“
[ROUTING] - Classify document type
    â†“
    â”œâ”€ Invoice â†’ [PROMPT CHAIN] â†’ Extract â†’ Validate â†’ Format
    â”œâ”€ Resume â†’ [PARALLEL] â†’ Extract contact + skills + experience
    â””â”€ Contract â†’ [ORCHESTRATOR-WORKERS] â†’ Analyze clauses dynamically
    â†“
All results â†’ [EVALUATOR-OPTIMIZER] â†’ Quality check â†’ Final output
```

**Pseudocode Practice:**
```pseudocode
FUNCTION advanced_document_processor(document):
    # Pattern 1: ROUTING
    doc_type = classify_document(document)  # Routing pattern

    # Pattern 2: Type-specific processing
    IF doc_type == "invoice":
        # PROMPT CHAINING pattern
        raw_data = extract_invoice_data(document)
        validated = validate_invoice(raw_data)
        result = format_invoice(validated)

    ELSE IF doc_type == "resume":
        # PARALLELIZATION pattern
        tasks = [
            extract_contact(document),
            extract_skills(document),
            extract_experience(document)
        ]
        result = PARALLEL_EXECUTE(tasks)

    ELSE IF doc_type == "contract":
        # ORCHESTRATOR-WORKERS pattern
        result = orchestrator_analyze_contract(document)

    # Pattern 3: EVALUATOR-OPTIMIZER (quality check)
    final_result = evaluator_optimizer(
        task="Ensure extracted data is accurate",
        initial_output=result
    )

    RETURN final_result
```

**Exercise (45 minutes):**
```
Problem: Build a content creation pipeline using ALL 5 patterns

Goal: Create a blog post from a topic

Requirements:
1. Classify topic category (tech/business/lifestyle) [ROUTING]
2. Research the topic (dynamic subtasks) [ORCHESTRATOR-WORKERS]
3. Generate: intro, body, conclusion separately [PARALLELIZATION]
4. Improve intro â†’ improve body â†’ improve conclusion [PROMPT CHAINING]
5. Quality check and iterate [EVALUATOR-OPTIMIZER]

Your Task:
1. Draw architecture diagram (which pattern where?)
2. Write pseudocode for entire pipeline
3. Show data flow between patterns
4. Identify error handling points
5. Plan fallback strategies
6. Estimate latency (which steps are slow?)
```

**Key Insight:**
- Simple task = 1 pattern
- Medium task = 2-3 patterns
- Complex task = 4-5 patterns
- **Start simple, add patterns as needed**

---

### **Day 11-14: Implementation Week**

**Now you CODE!**

**Day 11: Implement Prompt Chaining**
- Build the two-pass extraction we did (but understand it deeply now!)
- Test with different data
- Measure latency between approaches

**Day 12: Implement Routing**
- Build a customer query router
- 3+ categories with specialists
- Test classification accuracy

**Day 13: Implement Parallelization**
- Build multi-task analyzer
- Measure speedup from parallelization
- Handle timeout and errors

**Day 14: Implement Orchestrator + Reflection**
- Build research assistant with dynamic task breakdown
- Add reflection loop for quality
- Test with complex queries

---

## Phase 2: Framework Mastery (Weeks 3-4)

### **Day 15-21: LangGraph Week**

**Why LangGraph?**
- Graph-based workflow design
- State management built-in
- Perfect for complex, stateful agents

**Daily Structure:**
- **Day 15:** LangGraph basics (nodes, edges, state)
- **Day 16:** Build prompt chaining in LangGraph
- **Day 17:** Build routing in LangGraph (conditional edges)
- **Day 18:** Build parallelization in LangGraph
- **Day 19:** Build orchestrator pattern in LangGraph
- **Day 20:** Build reflection loop in LangGraph
- **Day 21:** Build multi-pattern system in LangGraph

**Pseudocode + Code Pattern:**
```pseudocode
# Day 15 Example: LangGraph State Machine

DEFINE STATE:
    - messages: list of messages
    - current_step: string
    - result: string

DEFINE GRAPH:
    - Node: "analyze" â†’ analyze_function()
    - Node: "extract" â†’ extract_function()
    - Node: "validate" â†’ validate_function()

    - Edge: start â†’ analyze
    - Edge: analyze â†’ extract
    - Edge: extract â†’ validate
    - Edge: validate â†’ end

FUNCTION analyze_function(state):
    # Do analysis
    state["result"] = "analyzed"
    RETURN state

# Then implement in Python with LangGraph
```

---

### **Day 22-28: AutoGen & CrewAI Week**

**Day 22-24: AutoGen (Multi-Agent Conversations)**
- Build conversational agents
- Agent-to-agent communication
- Group chat patterns

**Day 25-27: CrewAI (Role-Based Teams)**
- Define agent roles
- Assign tasks to crew members
- Coordinate team workflows

**Day 28: Framework Comparison**
- Same task in all 3 frameworks
- Compare code complexity
- Measure performance
- Choose your favorite

---

## Phase 3: Multi-Agent Systems (Weeks 5-6)

### **Key Concepts:**
- Agent communication protocols
- Task delegation strategies
- Conflict resolution
- Emergent behavior

**Week 5: Coordination Patterns**
- Leader-follower
- Peer-to-peer
- Hierarchical teams
- Democratic consensus

**Week 6: Real Systems**
- Build: Customer service team (5 agents)
- Build: Research assistant team (3 agents)
- Build: Code review team (4 agents)

---

## Phase 4: Autonomous Agents (Weeks 7-12)

### **Week 7: Memory Systems**
- Short-term memory (conversation)
- Long-term memory (RAG)
- Working memory (state)
- Semantic memory (knowledge graphs)

### **Week 8: Planning & Reasoning**
- Goal decomposition
- Plan generation
- Plan execution
- Plan adjustment (re-planning)

### **Week 9: Tool Use & Environment**
- Tool selection strategies
- API integration
- File system operations
- Database operations

### **Week 10: Self-Improvement**
- Performance tracking
- Strategy adjustment
- Learning from feedback
- Experience replay

### **Week 11: Production Deployment**
- Safety constraints
- Monitoring & observability
- Error recovery
- Cost optimization

### **Week 12: Capstone Project**

**Build: Fully Autonomous Research Agent**

Requirements:
- Takes research question
- Plans investigation strategy
- Executes research (web, papers, data)
- Validates findings
- Generates report
- Self-corrects errors
- Operates within budget constraints

**Deliverables:**
1. Complete architecture (all patterns used)
2. Pseudocode for entire system
3. Working implementation
4. Test results
5. Performance metrics
6. Cost analysis
7. Future improvements

---

## ðŸ“ Daily Practice Format

### Every Session (60 minutes):

**Minutes 0-10: Understand**
- Read the pattern/concept
- Review examples
- Note key principles

**Minutes 10-25: Plan (Pseudocode)**
- Work backwards from goal
- Identify which patterns apply
- Write pseudocode
- Draw architecture diagram

**Minutes 25-50: Build or Practice**
- Weeks 1-2: More pseudocode practice (no coding)
- Weeks 3+: Implement in code
- Test and iterate

**Minutes 50-60: Reflect & Document**
- What worked?
- What was hard?
- What did you learn?
- How does this connect to previous learning?

---

## ðŸŽ¯ Progress Tracking

### Weekly Self-Assessment (Rate 1-10):

**Weeks 1-2: Foundations**
- [ ] Can I write clear pseudocode?
- [ ] Can I identify pattern needed for a problem?
- [ ] Can I work backwards from desired output?
- [ ] Can I map dependencies?

**Weeks 3-4: Patterns**
- [ ] Can I recognize all 5 patterns?
- [ ] Can I implement each pattern?
- [ ] Can I combine patterns effectively?
- [ ] Can I choose the right pattern?

**Weeks 5-6: Frameworks**
- [ ] Can I build with LangGraph?
- [ ] Can I build with AutoGen?
- [ ] Can I build with CrewAI?
- [ ] Can I choose the right framework?

**Weeks 7-12: Mastery**
- [ ] Can I design multi-agent systems?
- [ ] Can I implement autonomous agents?
- [ ] Can I deploy to production?
- [ ] Can I optimize cost and performance?

**Mastery Goal:** 8+ on all items

---

## ðŸ“š Resources

### Official Guides:
1. **Anthropic:** "Building Effective Agents" - https://www.anthropic.com/research/building-effective-agents
2. **LangGraph:** Official docs - https://langchain-ai.github.io/langgraph/
3. **AutoGen:** Microsoft docs
4. **CrewAI:** Official tutorials

### Learn Alongside:
1. "AI Agents in LangGraph" - DeepLearning.AI
2. "Agentic AI with LangGraph" - Coursera
3. Anthropic cookbook examples
4. LangChain blog (agentic patterns)

### Practice Platforms:
- Build 1 agent per day (simple tasks)
- Contribute to open source agentic projects
- Join AI agent communities (Discord, Reddit)

---

## âœ… Completion Criteria

### You've achieved mastery when:

âœ… **Pattern Recognition**
- Can identify which pattern(s) fit any problem in 5 minutes
- Can explain trade-offs between patterns

âœ… **Pseudocode Mastery**
- Write clear pseudocode before any coding
- Others can implement from your pseudocode

âœ… **Framework Fluency**
- Build same agent in all 3 frameworks
- Choose right tool for the job

âœ… **System Design**
- Design multi-agent systems from scratch
- Plan for production (cost, scale, errors)

âœ… **Autonomous Thinking**
- Build agents that self-improve
- Deploy agents that operate safely

---

## ðŸš€ After Mastery

1. **Build in Public** - Share your agents on GitHub
2. **Teach Others** - Write tutorials, make videos
3. **Contribute** - Open source agentic projects
4. **Innovate** - Discover new patterns
5. **Deploy** - Production agentic systems

---

## ðŸ”‘ Key Principles (Review Daily)

1. **"Start simple, not complex"** - Anthropic
2. **"Patterns over frameworks"** - Composability
3. **"Plan before code"** - Pseudocode saves time
4. **"Test each layer"** - Bottom-up validation
5. **"Iterate, don't perfect"** - Ship and improve

---

## ðŸ“… Your Journey

**Start Date:** _______________
**Week 2 Complete:** _______________
**Week 4 Complete:** _______________
**Week 6 Complete:** _______________
**Week 12 Complete (MASTERY):** _______________

---

## ðŸŽ¯ Next Concept After This Session

Based on our progress, **next concept is: Day 8 - RAG Systems (Iteration 4 or new advanced topic)**

But first, let's solidify:
- âœ… Day 7 Complete: AI Resume Extractor (Two-Pass = Prompt Chaining!)
- âœ… Curriculum Created
- ðŸ”„ Ready for next learning module

---

**Remember:**

> "The most successful implementations were building with simple, composable patterns."
> â€” Anthropic, Building Effective Agents (2024)

**Think in patterns. Plan in pseudocode. Build with purpose.**

---

*Curriculum Version: 2025.1*
*Based on: Anthropic research + Industry best practices*
*Updated: December 16, 2025*
